{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3, Part I: Spark (25 points)\n",
    "\n",
    "\n",
    "You are expected to run this notebook on Google Cloud.  Please make sure the Jupyter menu above shows \"PySpark\" at the upper right, and not \"Python 3\".  If it says \"Python 3\" then go to the *Kernel* menu and choose *PySpark*.\n",
    "\n",
    "## Step 1: \"Big Data\": Transitive Closure over the Yelp Network\n",
    "\n",
    "In the previous assignment, you had built several primitives, including breadth-first search, to compute over the Yelp social graph.  Some of your computations were limited by the amount of computational resources available.  We will now try to explore the broader graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints: For early testing of your solutions to Step 1, you should probably just load a subset of this file into `graph_sdf` to validate your solutions.  Later go back, add the contents of the other files to graph_sdf, and re-run your solutions.  Make sure you rerun your code with the full `graph_sdf` before submitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1 Spark Initialization in Google DataProc\n",
    "\n",
    "Spark is already pre-initialized in DataProc.  You'll want to import some of the libraries, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2 Loading the Data\n",
    "\n",
    "Let’s read our social graph data from Yelp, which forms a directed graph.  Here, the set of nodes is also not specified; the assumption is that the only nodes that matter are linked to other nodes, and thus their IDs will appear in the set of edges.  For example, to load the file `input.txt` into a Spark DataFrame, you can use lines like the following. Note that we aren't running just `.txt` files in this assignment!\n",
    "\n",
    "```\n",
    "# Read lines from the text file, from Google Storage bucket my-bucket\n",
    "input_sdf = spark.read.format(\"txt\").load(\"gs://my-bucket/mypath/input.txt\")\n",
    "```\n",
    "\n",
    "We’ll use the suffix `_sdf` to represent “Spark DataFrame,” much as we used `_sdf` to denote a Spark DataFrame in Homework 2. You will load a `yelp_reviews_sdf` table from the `yelp_reviews2.csv` file from your HW2. You may find dataset descriptions [here](https://www.kaggle.com/yelp-dataset/yelp-dataset/version/6#yelp_review.csv).\n",
    "\n",
    "You'll need to upload this to Google Storage.  Uploading, and subsequently reading, this file may take a while. \n",
    "\n",
    "Add a function call (e.g., to `selectExpr()`) to rename the `user` and `business` nodes to `from_node` and `to_node`, to convert these to strings, respectively.\n",
    "\n",
    "Please name the tables as we have described in this assignment. This helps with our grading and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load yelp_reviews_sdf\n",
    "# Worth 3 points in total\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "# yelp_reviews_sdf = spark.read.load(\"gs://bucket-name-af/notebooks/yelp_review2.csv\",\n",
    "yelp_reviews_sdf = spark.read.load(\"/home/jovyan/work/hw2/data/yelp_review2.csv\",\n",
    "                     format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "yelp_reviews_sdf = yelp_reviews_sdf.selectExpr(\"user_id as from_node\", \"business_id as to_node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_node: string (nullable = true)\n",
      " |-- to_node: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just to show what it looks like\n",
    "yelp_reviews_sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell intentionally left blank ;-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  create a `graph_sdf` and save it in the Spark TempView so that you can call your transitive closure function on it here, regardless of whether that function is written in SparkSQL or Spark DataFrame function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[from_node: string, to_node: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Convert from yelp_reviews_sdf to a graph\n",
    "# Worth 2 points\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# YOUR CODE HERE\n",
    "graph_sdf = yelp_reviews_sdf\n",
    "\n",
    "# YOUR CODE HERE\n",
    "graph_sdf.createOrReplaceTempView('graph')\n",
    "\n",
    "graph_sdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert graph_sdf.count() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3. Transitive Closure of the Graph\n",
    "\n",
    "Now we would like to do the following: given a set of nodes, compute the set of all nodes reachable from these nodes.  This can be obtained via a type of transitive closure computation.\n",
    "\n",
    "Define a function `transitive_closure(graph_sdf, origins_sdf, depth)` that returns a Spark DataFrame.\n",
    "\n",
    "The result should be the set of all nodes from the `input graph_sdf` that are reachable via graph edges from the set of `origins_sdf`, in at most depth iterations (hops).  Both origins_sdf and the returned result should be DataFrames with a single attribute called node.  \n",
    "\n",
    "You should treat the edges in the `graph_sdf` as undirected edges!  You should iterate until you have either hit the maximum depth or the set of newly discovered (frontier) nodes is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints: this resembles your BFS algorithm from HW2, but you should take advantage of the opportunities to optimize.  Both the graph and the various node sets can easily have duplicates; you should make heavy use of duplicate removal (and repartition and cache) since you are only computing the set of reachable nodes. Also, to quickly check whether a DataFrame is empty, you can use something similar to the following. \n",
    "\n",
    "(You are not needed to update the following code block. Please start your code in the block containing `transitive_closure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdf_is_empty(sdf):\n",
    "    try:\n",
    "        sdf.take(1)\n",
    "        return False\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insert your code for `transitive_closure` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write the transitive_closure function\n",
    "# Will be worth 10 points in total\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def transitive_closure(graph, origin, depth):\n",
    "    # YOUR CODE HERE\n",
    "    G = graph.dropDuplicates()\n",
    "    G.createOrReplaceTempView('G')\n",
    "    G = G.cache()\n",
    "    \n",
    "    # initialize frontier & final DFs based on origin input\n",
    "    origin.createOrReplaceTempView('origin')\n",
    "    \n",
    "    final = origin\n",
    "    final.createOrReplaceTempView('final')\n",
    "    final = final.cache()\n",
    "\n",
    "    frontier = origin\n",
    "    frontier.createOrReplaceTempView('frontier')\n",
    "    frontier = frontier.cache()\n",
    "    \n",
    "    # iterate up to depth, or until frontier is empty\n",
    "    for i in range(0,depth):\n",
    "        # find neighbors\n",
    "        neigh1 = spark.sql('SELECT to_node as node FROM G '\\\n",
    "                            'WHERE EXISTS (SELECT * FROM frontier WHERE G.from_node=frontier.node)')\n",
    "        neigh2 = spark.sql('SELECT from_node as node FROM G '\\\n",
    "                            'WHERE EXISTS (SELECT * FROM frontier WHERE G.to_node=frontier.node)')\n",
    "        # add neighbors to final output\n",
    "        final = final.union(neigh1)\n",
    "        final = final.union(neigh2)\n",
    "        final = final.dropDuplicates()\n",
    "        final.createOrReplaceTempView('final')\n",
    "        final = final.repartition(10)\n",
    "        final = final.cache()\n",
    "        # update frontier\n",
    "        frontier = neigh1.union(neigh2)\n",
    "        frontier = frontier.dropDuplicates()\n",
    "        try: # see if frontier is empty, if so break the loop\n",
    "            frontier.take(1)\n",
    "            frontier.createOrReplaceTempView('frontier')\n",
    "            frontier = frontier.repartition(10)\n",
    "            frontier = frontier.cache()\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4 Testing transitive closure\n",
    "\n",
    "Compute a Spark DataFrame called `nodes_sdf` as follows:\n",
    "* Select from the graph those `from_node` values matching \"0\", \"1\", \"2\", \"3\", or \"4\".\n",
    "* Rename the column to `node`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create nodes_sdf\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "nodes_sdf = spark.sql('SELECT DISTINCT from_node FROM graph '\\\n",
    "                      'WHERE from_node=\"0\" OR from_node=\"1\" OR from_node=\"2\" OR from_node=\"3\" OR from_node=\"4\" ')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "nodes_sdf = nodes_sdf.selectExpr(\"from_node as node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# You should only have one column\n",
    "assert len(nodes_sdf.columns) == 1\n",
    "\n",
    "print (nodes_sdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a Spark DataFrame called `reachable_sdf` with the results of `transitive_closure(graph_sdf, nodes_sdf, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "reachable_sdf = transitive_closure(graph_sdf, nodes_sdf, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test case that we get a result\n",
    "assert reachable_sdf.take(1)[0].node != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and run two code Cells that call `count()` and then `show()`, respectively, on `reachable_sdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1236"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reachable_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                node|\n",
      "+--------------------+\n",
      "| When I need it c...|\n",
      "| Bottle and a Bow...|\n",
      "| more kawfee plea...|\n",
      "| we don't do that...|\n",
      "| Sauerkraut-Marti...|\n",
      "|     good riddance!\"|\n",
      "| looking very org...|\n",
      "| if different.  T...|\n",
      "| but don't go out...|\n",
      "|          I would..\"|\n",
      "| and now after th...|\n",
      "|                no.\"|\n",
      "| boring right? I ...|\n",
      "| and very persona...|\n",
      "|                  28|\n",
      "| but it's hard to...|\n",
      "| but with the has...|\n",
      "| I just took a ri...|\n",
      "|                  35|\n",
      "| you got to be at...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reachable_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
