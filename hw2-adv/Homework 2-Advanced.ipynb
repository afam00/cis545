{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2, Advanced Part: Iterative Spark Computations\n",
    "## Due October 15, 2018 by 10pm\n",
    "### Worth 20 points in total\n",
    "\n",
    "Building upon your experiences with graph data, we will now use Spark to compute PageRank.  Following our discussion of graphs, we will use an edge list, which is a variation of the adjacency list.\n",
    "\n",
    "# Step 5: PageRank\n",
    "\n",
    "Many of you may already know PageRank computation by its reputation:  it is used to measure the importance of a Web page.  (Contrary to popular belief, PageRank is named after Larry Page, not Web pages…)  PageRank is actually a tweaked version of a network centrality measure called *eigenvector centrality*.  One way to implement PageRank is as an iterative computation.  We take each graph node $x$ and in iteration 0 assign it a corresponding PageRank $p_x$:\n",
    "\n",
    "$p_x^0= 1 / N$\n",
    "\n",
    "where $N$ is the total number of nodes.\n",
    "\n",
    "Now in each iteration $i$ we recompute:\n",
    "\n",
    "$p_x^{(i)} = \\alpha * \\Sigma_{j \\in B(x)} (1 / N_j) p_j^{(i-1)} + \\beta$\n",
    "\n",
    "![Graph](graph.png)\n",
    "\n",
    "Where $B(x)$ is the set of nodes linking to node $x$, and $N_j$ is the outdegree of each such node $j$.  Typically, repeating the PageRank computation for a number of iterations (15 or so) results in convergence within an acceptable tolerance.  For this assignment we’ll assume $\\beta = 0.15$ and $\\alpha = 0.85$ (anecdoctally these are the most common values used in practice).\n",
    "\n",
    "*Example*. In the figure to the right, nodes $j_1$ and $j_2$ represent the back-link set $B(x)$ for node $x$.  $N_{j1}$ is 3 and $N_{j2}$ is 2.  Thus in each iteration $i$, we recompute the PageRank score for $x$ by adding half of the PageRank score for $j_2$ and a third of the PageRank score of $j_3$ (both from the previous iteration $i-1$).\n",
    "\n",
    "*Hint*.  Build some “helper” DataFrames.  We suggest at least 2 DataFrames, where the first is used the build the second, and the second is used in your solution:\n",
    "1. a DataFrame with each from_node and the proportion of weight it transfers to each outgoing edge.  For instance, if the from_node is node j then the proportion of weight should be $1/N_j$.\n",
    "2. a DataFrame, again with the from_node, each node it transfers weight to, and the proportion of weight computed in (1).  For instance, if the `from_node` is $j$ and the to_node is $x$, then the tuple should be $(j, x, 1/N_j)$.\n",
    "\n",
    "*Submission*. See the external document for submission information.  Remember to first do the basic part of **Homework 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1  Initialization and Marshalling\n",
    "\n",
    "### 5.1.1 Spark setup and data load\n",
    "\n",
    "Initialize PySpark as in the basic Homework 2.  Load `pr_graph.txt` as a text file with a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# TODO: Connect to Spark session as in Homework 2.\n",
    "# Then load the file\n",
    "\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('Graphs-HW2-adv').getOrCreate()\n",
    "\n",
    "pr_sdf = spark.read.text('pr_graph.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pr_sdf.count() != 19:\n",
    "    raise ValueError('Unexpected graph size')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Wrangling the Graph Data\n",
    "\n",
    "There are 3 columns in the file:\n",
    "* `from_node`\n",
    "* `to_node`\n",
    "* `reserved`\n",
    "\n",
    "You can ignore `reserved`.  Use Spark's *split()* function to update `pr_sdf` to have two columns, `from_node` and `to_node`.  Make each an integer.\n",
    "\n",
    "The split function in Spark, works similarly to the one in Python.  It can be called directly from Spark SQL (`select split(x,’ ’) …`) or by `import`ing `pyspark.sql.functions` and referring to the function in Python.\n",
    "\n",
    "You may need to cast your columns since they start off as strings.  In Python, you can call `my_sdf.column.cast(‘type’)` to convert data types.  In SQL it’s `SELECT CAST(my_sdf.column AS type).`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert pr_sdf into (from_node, to_node) with integer fields\n",
    "# Worth 5 points\n",
    "\n",
    "# YOUR CODE HERE\n",
    "split_col = F.split(pr_sdf['value'], ' ')\n",
    "pr_sdf = pr_sdf.withColumn('from_node', split_col.getItem(0).cast('int'))\n",
    "pr_sdf = pr_sdf.withColumn('to_node', split_col.getItem(1).cast('int'))\n",
    "pr_sdf = pr_sdf[['from_node','to_node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pr_sdf.take(20)\n",
    "\n",
    "if 'from_node' not in pr_sdf.columns:\n",
    "    raise KeyError('Unexpected column names')\n",
    "if 'to_node' not in pr_sdf.columns:\n",
    "    raise KeyError('Unexpected column names')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Basic PageRank\n",
    "\n",
    "Write the function `pagerank(G, num_iter)` which takes a graph DataFrame G corresponding to your graph, and runs for `num_iter` steps.  It should return a DataFrame with columns (`node_id`, `pagerank`).\n",
    "\n",
    "Initialize your PageRank values for each node in the “base case”.  Then, in each iteration, use the helper DataFrames to compute PageRank scores for each node in the next iteration.\n",
    "\n",
    "You will likely find it easier to express some of the computations in SparkSQL.  If you want to use spark.select, you may find it useful to use the Spark F.udf function to create functions that can be called over each row in the DataFrame.  You can create a function that returns a double as follows:\n",
    "\n",
    "```\n",
    "my_fn = F.udf(lambda x: f(x), DoubleType())\n",
    "```\n",
    "\n",
    "Then you can call it like:\n",
    "```\n",
    "\tmy_sdf.select(my_fn(my_arg)).alias(‘col_name’)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write the function\n",
    "# Worth 10 points\n",
    "def pagerank(G, num_iter):\n",
    "# YOUR CODE HERE\n",
    "\n",
    "    alpha = 0.85\n",
    "    beta = 0.15\n",
    "    G.createOrReplaceTempView('edges')\n",
    "\n",
    "    # calculate 1/n based on all unique nodes in either column\n",
    "    from_node = G.select('from_node')\n",
    "    to_node = G.select('to_node')\n",
    "    all_nodes = from_node.union(to_node)\n",
    "    num_nodes = all_nodes.distinct().count()\n",
    "    init_pr = 1/num_nodes\n",
    "\n",
    "    # initialize sdf of pageranks per node with value 1/n\n",
    "    temp_pr = np.repeat(init_pr,num_nodes)\n",
    "    temp_pr_df = pd.DataFrame(data={'node_id': list(range(1,num_nodes+1)),'pagerank':temp_pr})\n",
    "    mySchema = StructType([StructField('node_id', StringType(),False), StructField('pagerank', FloatType(), False)])\n",
    "    temp_pr_sdf = spark.createDataFrame(temp_pr_df,schema=mySchema)\n",
    "\n",
    "    # initialize weight matrix\n",
    "    weights = spark.sql('SELECT 1/count(*) as count FROM edges GROUP BY from_node ORDER BY from_node')\n",
    "    weights = weights.take(num_nodes)\n",
    "\n",
    "    # run for num_iter iterations\n",
    "    for i in range(0,num_iter):\n",
    "        temp_pr = np.zeros((num_nodes))\n",
    "        for a in range(0,num_nodes):\n",
    "            # first, find all nodes that connect to this node\n",
    "            nodes = spark.sql('SELECT from_node FROM edges WHERE to_node == '+str(a+1))\n",
    "\n",
    "            # for these nodes, calculate (1/N * PR) with PRs from last iteration\n",
    "            pr = np.zeros((nodes.count()))\n",
    "            for n in range(0,nodes.count()):\n",
    "                nodeID = nodes.take(nodes.count())[n][0]\n",
    "                pr[n] = (weights[nodeID-1][0]) * (temp_pr_sdf.take(num_nodes)[nodeID-1][1])\n",
    "                \n",
    "            # calculate PR for this iteration\n",
    "            temp_pr[a] = alpha * sum(pr) + beta\n",
    "        \n",
    "        # update PR sdf\n",
    "        temp_pr_df = pd.DataFrame(data={'node_id': list(range(1,num_nodes+1)),'pagerank':temp_pr})\n",
    "        mySchema = StructType([StructField('node_id', StringType(),False), StructField('pagerank', FloatType(), False)])\n",
    "        temp_pr_sdf = spark.createDataFrame(temp_pr_df,schema=mySchema)\n",
    "\n",
    "    return temp_pr_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|node_id|  pagerank|\n",
      "+-------+----------+\n",
      "|      4|  0.360847|\n",
      "|      1|0.44950113|\n",
      "|      6|0.48955232|\n",
      "|      3| 0.6585104|\n",
      "|      2| 0.6985616|\n",
      "|      5|0.81856054|\n",
      "|      7| 0.8622351|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pagerank(pr_sdf, 5).orderBy(\"pagerank\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Removal of Self-Loops\n",
    "\n",
    "The existing graph has a few self-loops.  Let's see what happens if you remove them.  For this one, take `pr_sdf` and remove all self-edges, creating `pr_no_loops_sdf`.  Run `pagerank(pr_no_loops_sdf, 5)`, sort in decreasing order by pagerank, and put the results in a list `pageranks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(node_id='2', pagerank=0.8792241811752319),\n",
       " Row(node_id='5', pagerank=0.7750296592712402),\n",
       " Row(node_id='7', pagerank=0.7306856513023376),\n",
       " Row(node_id='6', pagerank=0.5711990594863892),\n",
       " Row(node_id='1', pagerank=0.5176590085029602),\n",
       " Row(node_id='3', pagerank=0.46857768297195435),\n",
       " Row(node_id='4', pagerank=0.39539289474487305)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create pr_no_loops_sdf and feed it into pagerank.  \n",
    "# The final result should be an ordered list of Rows (nodes and pageranks) called pageranks.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# remove self-referencing edges\n",
    "pr_sdf.createOrReplaceTempView('pr')\n",
    "pr_no_loops_sdf = spark.sql('SELECT * FROM pr WHERE from_node != to_node')\n",
    "\n",
    "# run pagerank on resulting sdf\n",
    "pageranks_sdf = pagerank(pr_no_loops_sdf, 5).orderBy(\"pagerank\", ascending=False)\n",
    "\n",
    "# convert output sdf to a list\n",
    "pageranks = list(pageranks_sdf.collect())\n",
    "\n",
    "pageranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pageranks) != 7:\n",
    "    raise ValueError('Should have 7 nodes!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
